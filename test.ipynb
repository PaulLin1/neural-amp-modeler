{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09095b9e",
   "metadata": {},
   "source": [
    "# Notebook for experimenting with amp modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa3f28",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. Basic Wavenet implementation\n",
    "2. LSTM? Not sure how time series is handled but I saw it online\n",
    "3. Audio as input\n",
    "4. Audio as output\n",
    "5. fixing any bugs with audio. probably will be volume issues and pops etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80a059",
   "metadata": {},
   "source": [
    "## Goal for this notebook\n",
    "Make a capture of an amp. Model vs Profile. Profiling is when you model a amp completely. This includes eq and gain knobs and the way it responds to input. Profiling is a lot harder. Capturing just takes a snapshot of the amp at a certain setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb30533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependecies\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32513dd6",
   "metadata": {},
   "source": [
    "## Processing\n",
    "mu law encoding? compresses +-1 input to 0-225. It compresses the input for better training. Not like a audio compressor for production it doesnt alter the input just makes it easier to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709620a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu-law for compression\n",
    "# just a bunch of math that uses log compression to remove harsh sounds\n",
    "# might not use dont want to compress dynamics\n",
    "MU = 255\n",
    "def mu_law_encode(x, mu=MU):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        sign = torch.sign(x)\n",
    "        mag = torch.log1p(mu * x.abs()) / math.log1p(mu)\n",
    "        return ((sign * mag + 1) / 2 * mu).long()\n",
    "    else:\n",
    "        x = np.clip(x, -1, 1)\n",
    "        mag = np.log1p(mu * np.abs(x)) / np.log1p(mu)\n",
    "        encoded = np.sign(x) * mag\n",
    "        return ((encoded + 1) / 2 * mu).astype(np.int64)\n",
    "\n",
    "\n",
    "def mu_law_decode(encoded, mu=MU):\n",
    "    if isinstance(encoded, torch.Tensor):\n",
    "        enc = encoded.float()\n",
    "        x = 2 * (enc / mu) - 1\n",
    "        sign = torch.sign(x)\n",
    "        mag = (1 / mu) * ((1 + mu) ** x.abs() - 1)\n",
    "        return sign * mag\n",
    "    else:\n",
    "        x = 2 * (encoded.astype(np.float32) / mu) - 1\n",
    "        sign = np.sign(x)\n",
    "        mag = (1 / mu) * ((1 + mu) ** np.abs(x) - 1)\n",
    "        return sign * mag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f0527",
   "metadata": {},
   "source": [
    "# Wavenet\n",
    "\n",
    "Causal Conv\n",
    "A wavenet is a cnn but is causal. this means that it hides following input for training like llms. for llms you want to predict the next set of words, so having them as inputs would be cheating and make the task really easy you would just map the input to the output. for a task like image recognition, you look at the entire image for classification. cnn's are like this. so we have to modify it to be causal for audio prediction esque tassks\n",
    "\n",
    "resid layer\n",
    "kind of a theory thing. basically if you let input flow through the model without being modifided like passing it rhough a conv1d it makes training better. \n",
    "\n",
    "\n",
    "our model uses those 2 and some embedding layer and predition head. those are just feed forward networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Causal Conv1d wrapper\n",
    "# Just a conv layer thats causal meaning that it can see into the future\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.pad = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                              padding=self.pad, dilation=dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channel, timesteps)\n",
    "        out = self.conv(x)\n",
    "        if self.pad:\n",
    "            return out[:, :, :-self.pad]  # remove future context\n",
    "        return out\n",
    "\n",
    "# resid block. you dont need to understand this.\n",
    "# basically this is what makes wavenet a wave\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, residual_channels, skip_channels, kernel_size, dilation):\n",
    "        super().__init__()\n",
    "        self.filter_conv = CausalConv1d(residual_channels, residual_channels, kernel_size, dilation)\n",
    "        self.gate_conv = CausalConv1d(residual_channels, residual_channels, kernel_size, dilation)\n",
    "        self.res_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=1)\n",
    "        self.skip_conv = nn.Conv1d(residual_channels, skip_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, res, timesteps)\n",
    "        f = self.filter_conv(x)\n",
    "        g = self.gate_conv(x)\n",
    "        # gated activation unit\n",
    "        out = torch.tanh(f) * torch.sigmoid(g)\n",
    "        skip = self.skip_conv(out)\n",
    "        res = self.res_conv(out)\n",
    "        res = res + x  # residual connection\n",
    "        return res, skip # return pair of the output and skip conn\n",
    "\n",
    "class ConditionalWaveNet(nn.Module):\n",
    "    def __init__(self, n_quantize=256, residual_channels=32, skip_channels=64,\n",
    "                 kernel_size=2, dilations=None):\n",
    "        super().__init__()\n",
    "        if dilations is None:\n",
    "            # this is the wavenet patter youll see in the imgaes\n",
    "            # like th wave image\n",
    "            dilations = [1, 2, 4, 8, 16, 32] * 2\n",
    "\n",
    "        self.n_quantize = n_quantize\n",
    "        self.embedding = nn.Embedding(n_quantize, residual_channels)\n",
    "        self.causal_in = CausalConv1d(residual_channels, residual_channels, kernel_size=1)\n",
    "\n",
    "        # conditioning pathway (for clean input)\n",
    "        self.condition_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=1)\n",
    "\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(residual_channels, skip_channels, kernel_size, d)\n",
    "            for d in dilations\n",
    "        ])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.post1 = nn.Conv1d(skip_channels, skip_channels, kernel_size=1)\n",
    "        self.post2 = nn.Conv1d(skip_channels, n_quantize, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        # x: (target, input, sequence) (ampâ€™d)\n",
    "        # cond: conditioning clean sequence (same length)\n",
    "\n",
    "        x = self.embedding(x).permute(0, 2, 1).contiguous()\n",
    "        cond = self.embedding(cond).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        x = self.causal_in(x)\n",
    "        cond = self.condition_conv(cond)\n",
    "\n",
    "        skip_sum = 0\n",
    "        for block in self.res_blocks:\n",
    "            # inject conditioning\n",
    "            x, skip = block(x + cond)\n",
    "            skip_sum = skip_sum + skip if isinstance(skip_sum, torch.Tensor) else skip\n",
    "\n",
    "        out = self.relu(skip_sum)\n",
    "        out = self.post1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.post2(out)\n",
    "        return out\n",
    "\n",
    "# This is how i load in my wave fiels\n",
    "class AmpDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, clean_wave, amp_wave, seq_len):\n",
    "        assert len(clean_wave) == len(amp_wave), \"Input and target lengths must match\"\n",
    "        self.x = torch.tensor(clean_wave, dtype=torch.long)\n",
    "        self.y = torch.tensor(amp_wave, dtype=torch.long)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx:idx + self.seq_len]\n",
    "        y = self.y[idx + 1:idx + self.seq_len + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc88b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process input stuff\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# Process clean\n",
    "sr, waveform = wavfile.read(\"samples/clean/double stop.wav\")\n",
    "if waveform.ndim > 1:\n",
    "    waveform = waveform.mean(axis=1)\n",
    "\n",
    "waveform = waveform.astype(np.float32)\n",
    "waveform /= np.abs(waveform).max()\n",
    "clean_quant = mu_law_encode(waveform)\n",
    "\n",
    "# Process amped\n",
    "sr, waveform = wavfile.read(\"samples/real/double stop.wav\")\n",
    "if waveform.ndim > 1:\n",
    "    waveform = waveform.mean(axis=1)\n",
    "\n",
    "waveform = waveform.astype(np.float32)\n",
    "waveform /= np.abs(waveform).max()\n",
    "amp_quant = mu_law_encode(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AmpDataset(clean_quant, amp_quant, seq_len=512)\n",
    "dl = DataLoader(dataset, batch_size=1024, shuffle=True, drop_last=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConditionalWaveNet().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for epoch, (x, y) in enumerate(dl):\n",
    "    if epoch % 100:\n",
    "        print(f'{epoch} / {len(dl)}')\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    cond = x  # clean input (or use different clean signal tensor)\n",
    "    logits = model(x, cond)\n",
    "    # match sequence length\n",
    "    logits = logits[:, :, -y.shape[1]:]\n",
    "    loss = criterion(logits.permute(0, 2, 1).reshape(-1, model.n_quantize), y.reshape(-1))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e87c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to amp_output.wav\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "clean_tensor = torch.tensor(clean_quant, dtype=torch.long)\n",
    "amp_tensor   = torch.tensor(amp_quant, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_logits = model(clean_tensor.unsqueeze(0).to(device), clean_tensor.unsqueeze(0).to(device))\n",
    "    pred = torch.argmax(out_logits, dim=1).cpu().numpy().flatten()\n",
    "    audio = mu_law_decode(pred / 255.0)\n",
    "    audio = audio.astype(np.float32)\n",
    "    audio /= np.abs(audio).max() + 1e-8\n",
    "\n",
    "wavfile.write(\"amp_output.wav\", sr, (audio * 32767).astype(np.int16))\n",
    "print(\"Saved to amp_output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18e74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
