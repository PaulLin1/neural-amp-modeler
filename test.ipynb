{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09095b9e",
   "metadata": {},
   "source": [
    "# Notebook for experimenting with amp modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa3f28",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. Basic Wavenet implementation\n",
    "2. LSTM? Not sure how time series is handled but I saw it online\n",
    "3. Audio as input\n",
    "4. Audio as output\n",
    "5. fixing any bugs with audio. probably will be volume issues and pops etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80a059",
   "metadata": {},
   "source": [
    "## Goal for this notebook\n",
    "Make a capture of an amp. Model vs Profile. Profiling is when you model a amp completely. This includes eq and gain knobs and the way it responds to input. Profiling is a lot harder. Capturing just takes a snapshot of the amp at a certain setting. For this project, I will be capturing the Dumble clone amp from Neural DSP with these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb30533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5666db55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paullin/Documents/neural-amp-modeler/venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# import dependecies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32513dd6",
   "metadata": {},
   "source": [
    "## Wavenet\n",
    "A wavenet is a generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------\n",
    "# mu-law for compression\n",
    "# just a bunch of math that uses log compression to remove harsh sounds\n",
    "# might not use dont want to compress dynamics\n",
    "# ----------------------\n",
    "MU = 255\n",
    "def mu_law_encode(x, mu=MU):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        sign = torch.sign(x)\n",
    "        mag = torch.log1p(mu * x.abs()) / math.log1p(mu)\n",
    "        return ((sign * mag + 1) / 2 * mu).long()\n",
    "    else:\n",
    "        x = np.clip(x, -1, 1)\n",
    "        mag = np.log1p(mu * np.abs(x)) / np.log1p(mu)\n",
    "        encoded = np.sign(x) * mag\n",
    "        return ((encoded + 1) / 2 * mu).astype(np.int64)\n",
    "\n",
    "\n",
    "def mu_law_decode(encoded, mu=MU):\n",
    "    if isinstance(encoded, torch.Tensor):\n",
    "        enc = encoded.float()\n",
    "        x = 2 * (enc / mu) - 1\n",
    "        sign = torch.sign(x)\n",
    "        mag = (1 / mu) * ((1 + mu) ** x.abs() - 1)\n",
    "        return sign * mag\n",
    "    else:\n",
    "        x = 2 * (encoded.astype(np.float32) / mu) - 1\n",
    "        sign = np.sign(x)\n",
    "        mag = (1 / mu) * ((1 + mu) ** np.abs(x) - 1)\n",
    "        return sign * mag\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Causal Conv1d wrapper\n",
    "# Just a conv layer thats causal meaning that it can see into the future\n",
    "# ----------------------\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.pad = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                              padding=self.pad, dilation=dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channel, timesteps)\n",
    "        out = self.conv(x)\n",
    "        if self.pad:\n",
    "            return out[:, :, :-self.pad]  # remove future context\n",
    "        return out\n",
    "\n",
    "# resid block. you dont need to understand this.\n",
    "# basically this is what makes wavenet a wave\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, residual_channels, skip_channels, kernel_size, dilation):\n",
    "        super().__init__()\n",
    "        self.filter_conv = CausalConv1d(residual_channels, residual_channels, kernel_size, dilation)\n",
    "        self.gate_conv = CausalConv1d(residual_channels, residual_channels, kernel_size, dilation)\n",
    "        self.res_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=1)\n",
    "        self.skip_conv = nn.Conv1d(residual_channels, skip_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, res, timesteps)\n",
    "        f = self.filter_conv(x)\n",
    "        g = self.gate_conv(x)\n",
    "        # gated activation unit\n",
    "        out = torch.tanh(f) * torch.sigmoid(g)\n",
    "        skip = self.skip_conv(out)\n",
    "        res = self.res_conv(out)\n",
    "        res = res + x  # residual connection\n",
    "        return res, skip\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# WaveNet model\n",
    "# ----------------------\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, n_quantize=MU + 1, residual_channels=32, skip_channels=64,\n",
    "                 kernel_size=2, dilations=None):\n",
    "        super().__init__()\n",
    "        if dilations is None:\n",
    "            # this is the wavenet patter youll see in the imgaes\n",
    "            dilations = [1, 2, 4, 8, 16, 32] * 2\n",
    "\n",
    "        self.n_quantize = n_quantize\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "\n",
    "        # embed input\n",
    "        self.embedding = nn.Embedding(n_quantize, residual_channels)\n",
    "        self.causal_in = CausalConv1d(residual_channels, residual_channels, kernel_size=1)\n",
    "\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(residual_channels, skip_channels, kernel_size, d)\n",
    "            for d in dilations\n",
    "        ])\n",
    "\n",
    "        # post-processing\n",
    "        # just stacking conv and relu layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.post1 = nn.Conv1d(skip_channels, skip_channels, kernel_size=1)\n",
    "        self.post2 = nn.Conv1d(skip_channels, n_quantize, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time)\n",
    "        x = self.embedding(x).permute(0, 2, 1).contiguous()\n",
    "        x = self.causal_in(x)\n",
    "\n",
    "        skip_sum = 0\n",
    "        for block in self.res_blocks:\n",
    "            x, skip = block(x)\n",
    "            skip_sum = skip_sum + skip if isinstance(skip_sum, torch.Tensor) else skip\n",
    "\n",
    "        out = self.relu(skip_sum)\n",
    "        out = self.post1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.post2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Dataset helper (takes raw waveform arrays already quantized)\n",
    "# ----------------------\n",
    "class WaveDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, quantized_wave, seq_len):\n",
    "        if isinstance(quantized_wave, torch.Tensor):\n",
    "            self.data = quantized_wave.long()\n",
    "        elif isinstance(quantized_wave, np.ndarray):\n",
    "            self.data = torch.tensor(quantized_wave, dtype=torch.long)\n",
    "        else:\n",
    "            self.data = torch.tensor(np.array(quantized_wave), dtype=torch.long)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + 1:idx + self.seq_len + 1]\n",
    "        return x, y\n",
    "\n",
    "# ----------------------\n",
    "# Simple autoregressive generation (slow, sample-by-sample)\n",
    "# ----------------------\n",
    "@torch.no_grad()\n",
    "def generate(model, device, initial_sequence, gen_len=16000, temperature=1.0):\n",
    "    \"\"\"\n",
    "    initial_sequence: 1D tensor of quantized values to prime the model (length L)\n",
    "    returns quantized waveform of length L + gen_len (torch.LongTensor)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = initial_sequence.clone().long().to(device)\n",
    "    for _ in range(gen_len):\n",
    "        # feed last chunk (the model is fully convolutional; feeding whole sequence is fine but grows)\n",
    "        x = generated[-1024:].unsqueeze(0)  # (1, T)\n",
    "        logits = model(x)  # (1, n_quantize, T)\n",
    "        logits_last = logits[:, :, -1].squeeze(0)  # (n_quantize)\n",
    "        probs = F.softmax(logits_last / max(1e-8, temperature), dim=-1)\n",
    "        sample = torch.multinomial(probs, 1)\n",
    "        generated = torch.cat([generated, sample.squeeze(0)])\n",
    "    return generated.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 8000\n",
    "sec = 5\n",
    "t = np.linspace(0, sec, sr * sec, endpoint=False)\n",
    "sine = 0.6 * np.sin(2 * np.pi * 220 * t)  # 220 Hz tone\n",
    "quant = mu_law_encode(sine)  # ints 0..255\n",
    "\n",
    "\n",
    "seq_len = 512\n",
    "dataset = WaveDataset(quant, seq_len=seq_len)\n",
    "dl = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WaveNet(n_quantize=MU + 1, residual_channels=32, skip_channels=64).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04f66b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# generate 1s of audio primed with a short seed\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device)\u001b[39m\n\u001b[32m    181\u001b[39m loss = criterion(logits.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m).reshape(B * T, C),\n\u001b[32m    182\u001b[39m                  y.reshape(B * T))\n\u001b[32m    183\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m    186\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-amp-modeler/venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-amp-modeler/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neural-amp-modeler/venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for x, y in dl:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        B, C, T = logits.shape\n",
    "        loss = criterion(logits.permute(0, 2, 1).reshape(B * T, C),\n",
    "                         y.reshape(B * T))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * B\n",
    "    loss =  total_loss / len(dl.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e87c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1s of audio primed with a short seed\n",
    "seed = torch.from_numpy(quant[:256]).long()\n",
    "out_quant = generate(model, device, seed, gen_len=sr)\n",
    "out_wave = mu_law_decode(out_quant.numpy())\n",
    "# save or listen using soundfile or scipy (not included here)\n",
    "print(\"Generated waveform (first 10 samples):\", out_wave[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18e74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
