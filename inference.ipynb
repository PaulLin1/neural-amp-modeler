{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5666db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependecies\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import math\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.io.wavfile import write\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6f8ed5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(path, target_rms=0.1):\n",
    "    sr, waveform = wavfile.read(path)\n",
    "    if waveform.ndim > 1:\n",
    "        waveform = waveform.mean(axis=1)\n",
    "    waveform = waveform.astype(np.float32)\n",
    "    waveform -= np.mean(waveform)\n",
    "    \n",
    "    # Normalize amplitude and prevent clipping\n",
    "    waveform /= (np.max(np.abs(waveform)) + 1e-7)\n",
    "    waveform *= 0.99\n",
    "    \n",
    "    # High-pass filter to remove DC / subsonic\n",
    "    b, a = butter(1, 20 / (sr / 2), btype='highpass')\n",
    "    waveform = filtfilt(b, a, waveform)\n",
    "    \n",
    "    # RMS normalization\n",
    "    def rms(x): return np.sqrt(np.mean(x**2))\n",
    "    waveform *= target_rms / (rms(waveform) + 1e-9)\n",
    "    \n",
    "    return sr, waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e21a8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLayer(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, dilation):\n",
    "        super().__init__()\n",
    "        self.left_pad = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(channels, channels, kernel_size, dilation=dilation)\n",
    "\n",
    "        # optional 1x1 conv for residual\n",
    "        self.res_conv = nn.Conv1d(channels, channels, 1)\n",
    "        # optional 1x1 conv for skip\n",
    "        self.skip_conv = nn.Conv1d(channels, channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_padded = F.pad(x, (self.left_pad, 0))\n",
    "        out = torch.tanh(self.conv(x_padded))\n",
    "\n",
    "        skip = self.skip_conv(out)\n",
    "        res = self.res_conv(out) + x  # residual connection\n",
    "        return res, skip\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, channels=16, kernel_size=3, dilations=None):\n",
    "        super().__init__()\n",
    "        self.dilations = dilations if dilations is not None else [2 ** i for i in range(10)]\n",
    "        self.input_proj = nn.Conv1d(in_channels, channels, 1)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            CausalLayer(channels, kernel_size, d)\n",
    "            for d in self.dilations\n",
    "        ])\n",
    "\n",
    "        self.output_proj = nn.Conv1d(channels, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        skip_connections = []\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x, skip = block(x)\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "        # sum all skip connections and apply final projection\n",
    "        x = sum(skip_connections)\n",
    "        x = self.output_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "120f74f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_871943/1130839318.py:2: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sr, waveform = wavfile.read(path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr, clean_quant = preprocess_audio(\"riff_clean.wav\")\n",
    "sr, amp_quant   = preprocess_audio(\"scale_amp.wav\")\n",
    "\n",
    "clean_quant = torch.from_numpy(clean_quant.copy()).to(torch.float32)\n",
    "amp_quant = torch.from_numpy(amp_quant.copy()).to(torch.float32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = WaveNet().to(device)\n",
    "\n",
    "# state_dict = torch.load(\"model.pth\", map_location=\"cuda\")  # map_location if not using GPU\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "state_dict = torch.load(\"model.pth\", map_location=device)\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# Only keep keys that match in size\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "\n",
    "model_dict.update(filtered_state_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "14d9336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(clean_quant.unsqueeze(0).to(device))\n",
    "# q = mu_law_decode(amp_quant)\n",
    "write(\"riff_clean.wav.wav\", sr, (res * 32767).squeeze(0).detach().cpu().numpy().astype(np.int16))\n",
    "# write(\"outpu11.wav\", sr, (amp_quant * 32767).detach().cpu().numpy().astype(np.int16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming clean_tensor is [seq_len] or [batch_size, seq_len]\n",
    "# tensor_np = pred_np\n",
    "print(q.shape)\n",
    "plt.figure(figsize=(12, 4))\n",
    "# plt.plot(mu_law_decode(y_pred.squeeze(0).numpy()), color='blue')\n",
    "plt.plot(q, color='blue')\n",
    "\n",
    "plt.title(\"Clean Tensor Waveform\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Amplitude (quantized)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
